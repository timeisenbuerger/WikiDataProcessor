{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show warnings only once\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Callianira hexagona</td>\n",
       "      <td>Callianira hexagona is a species of Ctenophora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ceratomyxa hooperi</td>\n",
       "      <td>Ceratomyxa hooperi is a myxosporean parasite t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bee removal</td>\n",
       "      <td>Bee removal is the process of removing bees fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Petalonamae</td>\n",
       "      <td>The Petalonamae are a proposed extinct group o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knacker</td>\n",
       "      <td>A Knacker (), Knackerman or Knacker Man, is a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title                                            content\n",
       "0  Callianira hexagona  Callianira hexagona is a species of Ctenophora...\n",
       "1   Ceratomyxa hooperi  Ceratomyxa hooperi is a myxosporean parasite t...\n",
       "2          Bee removal  Bee removal is the process of removing bees fr...\n",
       "3          Petalonamae  The Petalonamae are a proposed extinct group o...\n",
       "4              Knacker  A Knacker (), Knackerman or Knacker Man, is a ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "filepath = current_dir + '/csv/needed_article_titles_and_contents.csv'\n",
    "df = pd.read_csv(filepath, delimiter = \";\", names = [\"title\", \"content\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "If you see the following window for the first time, please select \"all\" from the list and click \"download\". This may take a while. In consecutive runs you can simply close the window immediately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programme\\Miniconda3\\envs\\wikidata\\lib\\site-packages\\nltk\\decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load nltk\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Callianira', 'hexagona', 'is', 'a', 'species', 'of', 'Ctenophora', 'from', 'the', 'Mertensiidae']\n"
     ]
    }
   ],
   "source": [
    "# tokenize documents\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "documents = df[\"content\"].values\n",
    "\n",
    "def tokenize(documents): \n",
    "    return np.array([nltk.word_tokenize(document) for document in documents])\n",
    "\n",
    "tokens = tokenize(documents)\n",
    "print(tokens[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pos tagging\n",
    "def tag_pos(documents): \n",
    "    return np.array([nltk.pos_tag(document) for document in documents])\n",
    "\n",
    "pos_tags = tag_pos(tokens)\n",
    "print(pos_tags[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter nouns\n",
    "def filter_nouns(documents): \n",
    "    return np.array([[pos_tag[0].lower() for pos_tag in document if pos_tag[1][0] == \"N\"] for document in documents])\n",
    "\n",
    "nouns = filter_nouns(pos_tags)    \n",
    "print(nouns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def lemmatize(documents): \n",
    "    return np.array([[wn.morphy(noun) for noun in document if wn.morphy(noun) != None]for document in documents])\n",
    "\n",
    "lemmas = lemmatize(nouns)\n",
    "print(lemmas[0]) \n",
    "    \n",
    "def map_frequencies(documents): \n",
    "    frequencies = {}\n",
    "    for document in documents: \n",
    "        for word in document: \n",
    "            if word in frequencies: \n",
    "                frequencies[word] += 1\n",
    "            else: \n",
    "                frequencies[word] = 1\n",
    "    return frequencies\n",
    "\n",
    "# lemma_count = dict(zip(np.unique(np.hstack(lemmas).flatten(), return_counts = True)))\n",
    "lemma_count = map_frequencies(lemmas)\n",
    "print(lemma_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1: Most frequent nouns\n",
    "We expect words like \"animal\", \"behavior\", \"habitat\" and \"diet\" to be the most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "lemma_count_subset = {k: v for k, v in lemma_count.items() if v > 800}\n",
    "sorted_lemma = dict(sorted(lemma_count_subset.items(), key=lambda x: x[1]))\n",
    "\n",
    "labels = list(sorted_lemma.keys())\n",
    "values = list(sorted_lemma.values())\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.title(\"Most frequent nouns\", fontdict={'fontsize':16})\n",
    "plt.bar(labels, values)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()\n",
    "\n",
    "lemma_count_subset = {k: v for k, v in lemma_count.items() if v > 800 and v < 4000}\n",
    "sorted_lemma_n = dict(sorted(lemma_count_subset.items(), key=lambda x: x[1]))\n",
    "\n",
    "labels = list(sorted_lemma_n.keys())\n",
    "values = list(sorted_lemma_n.values())\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.title(\"Without \\\"animal\\\" and \\\"species\\\"\", fontdict={'fontsize':16})\n",
    "plt.bar(labels, values)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Hypothesis 1\n",
    "We were correct that \"animal\" belongs to the most frequent nouns, but we missed \"species\", the second most frequent noun. A lot of animal names like \"bird\", \"dog\" and \"fish\" are among the most frequent nouns as well. \"diet\" wasn't among the top words, but a similar one (\"food\") is. As we guessed, \"behavior\" also is one of the the top words, but \"population\" would also have been a plausible guess which we should have taken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter stopwords\n",
    "def filter_stopwords(documents): \n",
    "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    docs = []\n",
    "    for document in documents: \n",
    "        doc = []\n",
    "        for word in document: \n",
    "            lemma = wn.morphy(word)\n",
    "            condition = lemma not in en_stop and lemma != None and len(lemma) > 2\n",
    "            if condition: \n",
    "                doc.append(lemma)\n",
    "        docs.append(doc)\n",
    "    return np.array(docs)\n",
    "                \n",
    "filtered = filter_stopwords(tokens)\n",
    "print(filtered[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for LDA\n",
    "import gensim \n",
    "dictionary = gensim.corpora.Dictionary(filtered)\n",
    "corpus = [dictionary.doc2bow(document) for document in filtered]\n",
    "\n",
    "print(dictionary)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 2: Topic Clusters\n",
    "We expect topics to emerge from different types and contexts of animals like \"pet\" which includes \"cat\", \"dog\" and \"human\" and its' counterpart: wild animals with words like \"tiger\", \"predator\" and \"wild\"; \"cattle\" consisting of \"cow\", \"pig\" and \"livestock\"; and also clusters separating mammals from reptiles and terrestrial from aquatic animals. And since \"bat\" and \"bird\" were among the most frequent nouns, a cluster describing flying animals could emerge as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word = dictionary, passes = 15, random_state = 1604)\n",
    "topics = ldamodel.print_topics(num_words = 4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize topics (using first three levels)\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics = False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Hypothesis 2\n",
    "Though cluster number 3 includes the words \"virus\", \"pain\", \"disease\" and \"infection\", describing cellular life which we didn't take into consideration. Also, cluster 9 roughly resembles what we described as wild animals, as it includes the words \"prey\" and \"predator\". \n",
    "Cluster 4 covers what we called aquatic animals, with words like \"fish\", \"dolphin\" and \"waters\". \n",
    "And as we expected, a cluster about flying animals emerged (cluster 6) with words including \"owl\", \"bird\", \"nest\", \"eggs\" but not \"bat\", so it only describes birds and not flying animals in general. \n",
    "Cluster 8 is pretty niche since it seems to describe animals from dry areas like deserts with words like \"snake\" and \"coyote\". Right next to it is cluster 5, describing livestock as we guessed. It includes the words \"cattle\", \"horse\" and \"donkey\" but not \"cow\" and \"pig\". \n",
    "\n",
    "cellular 5\n",
    "wild\n",
    "pet 4\n",
    "water 7\n",
    "air 6\n",
    "cattle 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using only topics from the first layer\n",
    "level_map_path = \"C:/Users/Anita/Desktop/data/level_map.csv\"\n",
    "level_map = pd.read_csv(level_map_path, delimiter = \";\", names = [\"level\", \"titles\"])\n",
    "level_map[\"level\"] = [0,1,2]\n",
    "level_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize named entity distribution\n",
    "def ner(documents): \n",
    "    named_entities = []\n",
    "    for document in documents: \n",
    "        for sentence in nltk.sent_tokenize(document): \n",
    "            for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence))): \n",
    "                if hasattr(chunk, 'label'): \n",
    "                    chunkname = chunk.leaves()[0][0]\n",
    "                    named_entities.append(chunkname)\n",
    "    return np.array(named_entities)\n",
    "                \n",
    "named_entities = ner(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_frequencies_ner(named_entities): \n",
    "    frequencies = {}\n",
    "    for named_entity in named_entities: \n",
    "        if named_entity in frequencies: \n",
    "            frequencies[named_entity] += 1\n",
    "        else: \n",
    "            frequencies[named_entity] = 1\n",
    "    return frequencies\n",
    "            \n",
    "ne_count = map_frequencies_ner(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 3: Most frequent named entities\n",
    "It is unlikely that a specific animal name is one of the top named entities. Rather, it could be locations since many different species live in one region. This could be described in terms of country or continent names like \"Africa\", \"Australia\" and \"U.S.\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_count_subset = {k: v for k, v in ne_count.items() if v > 100}\n",
    "sorted_ne = dict(sorted(ne_count_subset.items(), key=lambda x: x[1]))\n",
    "\n",
    "labels = list(sorted_ne.keys())\n",
    "values = list(sorted_ne.values())\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.title(\"Most common named entities\", fontdict={'fontsize':16})\n",
    "plt.bar(labels, values)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Hypothesis 3\n",
    "The guess that locations are among the most frequent named entities was right, in fact, there not a single animal name included. We though that maybe a few would be listed. We assumed that continents and rough region descriptions would be more common than countries, and \"Canada\" is the most frequent occuring named entity, followed by \"African\", \"North\", \"South\", and \"Western\". The last three probably belong to other country names, so they should be discarded. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
